import streamlit as st
from src.collection.google_scholar import GoogleScholarScraper
from src.collection.arxiv_api import ArxivScraper
from src.collection.pubmed_api import PubMedScraper
from src.collection.crossref_api import CrossrefScraper
from src.database import get_db, Article, SearchSession, init_db
from src.advanced_sorting import AdvancedRanker
import threading
import pandas as pd
import re

st.set_page_config(page_title="Recherche", layout="wide")

# === DESIGN SYSTEM PREMIUM ===
from src.ui_utils import load_premium_css
load_premium_css()

st.title("üîç Recherche d'Articles")

init_db()
db = next(get_db())

# Onglets pour s√©parer les deux modes
tab1, tab2 = st.tabs(["Mode Automatique", "Mode Avanc√©"])

# ==================== ONGLET 1: MODE AUTOMATIQUE ====================
with tab1:
    st.info("Mode recommand√© : Recherche sur arXiv avec enrichissement automatique des m√©tadonn√©es")
    
    with st.form("auto_search"):
        # R√©cup√©rer la derni√®re recherche ou valeur par d√©faut
        default_query = st.session_state.get('active_session_query', "machine learning")
        query = st.text_input("Mots-cl√©s de recherche", default_query)
        
        # Mode de recherche
        search_mode = st.radio(
            "Mode de recherche",
            ["Nombre fixe", "MAX (Tous les articles disponibles)"],
            help="Mode MAX : R√©cup√®re TOUS les articles disponibles (peut prendre du temps)"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if search_mode == "Nombre fixe":
                num_results = st.slider("Nombre de r√©sultats arXiv", 5, 5000, 20)
            else:
                # Mode MAX
                enable_cap = st.checkbox("Plafond de s√©curit√©", value=True, 
                                        help="Limite le nombre max d'articles pour √©viter les recherches trop longues")
                if enable_cap:
                    cap_value = st.select_slider(
                        "Plafond maximum",
                        options=[100, 500, 1000, 2000, 5000, 10000],
                        value=1000
                    )
                    num_results = cap_value
                    st.caption(f"‚ö†Ô∏è Max {cap_value} articles")
                else:
                    num_results = 999999  # Valeur "illimit√©e"
                    st.warning("‚ö†Ô∏è Aucune limite ! Peut prendre plusieurs heures.")
        
        with col2:
            enrich = st.checkbox("Enrichir avec PubMed/Crossref (DOIs)", value=True, 
                                help="Cherche les m√™mes articles sur PubMed/Crossref pour obtenir les DOIs manquants")
        
        
        submitted = st.form_submit_button("Lancer la recherche automatique")
    
    if submitted:
        if not query:
            st.error("Veuillez entrer une requ√™te de recherche.")
        else:
            progress = st.progress(0)
            status = st.empty()
            
            # Phase 1: arXiv (principal)
            status.text("Phase 1/3 : Recherche arXiv avec t√©l√©chargement PDF...")
            progress.progress(0.1)
            
            arxiv = ArxivScraper()
            arxiv_results = arxiv.search(query, max_results=num_results)
            
            progress.progress(0.4)
            
            # Cr√©er session arXiv
            session = SearchSession(
                query=f"{query} [arXiv]",
                num_results=len(arxiv_results),
                successful_downloads=sum(1 for r in arxiv_results if r.get('extraction_status') == 'SUCCESS'),
                status='ACTIVE'
            )
            db.add(session)
            db.commit()
            db.refresh(session)
            
            # Debug session ID
            # print(f"DEBUG: Session cr√©√©e ID={session.id} pour {len(arxiv_results)} r√©sultats")
            
            # PERSISTANCE SESSION
            st.session_state.active_session_id = session.id
            st.session_state.active_session_query = query
            st.toast(f"Session active : {query}")

            # Sauvegarder arXiv
            status.text("Phase 2/3 : Sauvegarde des articles arXiv...")
            for result in arxiv_results:
                # D√©duplication par DOI ou titre
                existing_article = None
                
                if result.get('doi'):
                    existing_article = db.query(Article).filter(Article.doi == result['doi']).first()
                
                if not existing_article:
                    normalized_title = re.sub(r'[^\w\s]', '', result['title'].lower())
                    all_articles = db.query(Article).all()
                    for art in all_articles:
                        art_normalized = re.sub(r'[^\w\s]', '', art.title.lower())
                        if art_normalized == normalized_title:
                            existing_article = art
                            break
                
                if existing_article:
                    # Si l'article existe, on le lie √† la NOUVELLE session pour qu'il apparaisse
                    existing_article.search_session_id = session.id
                    # On met √† jour le statut si n√©cessaire (optionnel, on garde l'historique)
                    # existing_article.status = "IDENTIFIED" 
                else:
                    article = Article(
                        title=result['title'],
                        authors=result.get('authors'),
                        year=result.get('year'),
                        source=result.get('source'),
                        link=result.get('link'),
                        doi=result.get('doi'),
                        abstract=result.get('abstract'),
                        pdf_path=result.get('pdf_path'),
                        full_text=result.get('full_text'),
                        text_extraction_status=result.get('extraction_status', 'NOT_ATTEMPTED'),
                        extraction_method=result.get('extraction_method'),
                        status="IDENTIFIED",
                        search_session_id=session.id
                    )
                    db.add(article)
            
            db.commit()
            progress.progress(0.7)
            
            # Phase 3: Enrichissement (optionnel)
            enriched_count = 0
            if enrich:
                status.text("Phase 3/3 : Enrichissement PubMed/Crossref (DOIs)...")
                
                # Chercher seulement les articles arXiv sans DOI
                arxiv_no_doi = [r for r in arxiv_results if not r.get('doi')]
                
                if arxiv_no_doi:
                    st.caption(f"Recherche de DOIs pour {len(arxiv_no_doi)} articles...")
                    
                    # Crossref (meilleur pour les DOIs)
                    try:
                        crossref = CrossrefScraper()
                        crossref_results = crossref.search(query, max_results=20)
                        
                        # Matcher par titre
                        for arxiv_art in arxiv_no_doi:
                            arxiv_title_norm = re.sub(r'[^\w\s]', '', arxiv_art['title'].lower())
                            
                            for cross_art in crossref_results:
                                cross_title_norm = re.sub(r'[^\w\s]', '', cross_art['title'].lower())
                                
                                if arxiv_title_norm == cross_title_norm and cross_art.get('doi'):
                                    # Mettre √† jour l'article dans la BDD
                                    db_article = db.query(Article).filter(
                                        Article.title == arxiv_art['title']
                                    ).first()
                                    
                                    if db_article and not db_article.doi:
                                        db_article.doi = cross_art['doi']
                                        enriched_count += 1
                                        break
                        
                        db.commit()
                    except Exception as e:
                        st.warning(f"Enrichissement Crossref √©chou√©: {e}")
            
            progress.progress(1.0)
            status.text("Recherche termin√©e!")
            
            # R√©sum√©
            st.success(f"**{len(arxiv_results)} articles arXiv** trouv√©s")
            
            # Statistiques d√©taill√©es
            col_a, col_b, col_c, col_d = st.columns(4)
            col_a.metric("PDFs t√©l√©charg√©s", 
                        sum(1 for r in arxiv_results if r.get('pdf_path')))
            col_b.metric("Texte extrait", 
                        sum(1 for r in arxiv_results if r.get('extraction_status') == 'SUCCESS'))
            col_c.metric("DOIs enrichis", enriched_count)
            
            # Nombre total en BDD
            total_in_db = db.query(Article).count()
            col_d.metric("üìö Total en BDD", total_in_db, 
                        delta=f"+{len(arxiv_results)}" if len(arxiv_results) > 0 else "0")
            
            st.info("Consultez l'onglet 'Base de donn√©es' pour voir les articles.")
            
            # --- Lancement Analyse IA en arri√®re-plan ---
            st.toast("Lancement de l'analyse IA en arri√®re-plan...")
            
            def run_background_analysis(article_ids, query):
                ranker = AdvancedRanker()
                ranker.process_batch_and_update_db(article_ids, query)
            
            # R√©cup√©rer les IDs des nouveaux articles
            new_articles = db.query(Article).filter(Article.search_session_id == session.id).all()
            new_ids = [a.id for a in new_articles]
            
            if new_ids:
                # Lancer IA
                thread = threading.Thread(target=run_background_analysis, args=(new_ids, query))
                thread.start()
                
                # Lancer r√©cup√©ration PDFs manquants
                from src.pdf_retriever import auto_retrieve_missing_pdfs
                pdf_thread = threading.Thread(target=auto_retrieve_missing_pdfs, args=(session.id, 50))
                pdf_thread.start()
                
                st.toast("‚úì Analyse IA + R√©cup√©ration PDFs d√©marr√©es en arri√®re-plan !")


# ==================== ONGLET 2: MODE AVANC√â ====================
with tab2:
    st.warning("Mode avanc√© : S√©lection manuelle des sources (PubMed/Crossref ne fournissent pas de texte complet)")
    
    with st.form("advanced_search"):
        query_adv = st.text_input("Mots-cl√©s de recherche", "machine learning", key="adv_query")
        
        col1, col2 = st.columns(2)
        
        with col1:
            sources = st.multiselect(
                "Sources √† interroger",
                ["Google Scholar", "arXiv", "PubMed", "Crossref"],
                default=["arXiv"],
                help="arXiv: texte complet | PubMed/Crossref: m√©tadonn√©es seulement"
            )
        
        with col2:
            num_results_adv = st.slider("Nombre de r√©sultats par source", 5, 5000, 20, key="adv_num")
        
        submitted_adv = st.form_submit_button("Lancer la recherche avanc√©e")
    
    if submitted_adv:
        if not query_adv or not sources:
            st.error("Veuillez entrer une requ√™te et s√©lectionner au moins une source.")
        else:
            all_results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            scrapers = {
                "Google Scholar": GoogleScholarScraper(),
                "arXiv": ArxivScraper(),
                "PubMed": PubMedScraper(),
                "Crossref": CrossrefScraper()
            }
            
            for idx, source_name in enumerate(sources):
                status_text.text(f"Interrogation de {source_name}...")
                progress_bar.progress((idx) / len(sources))
                
                try:
                    scraper = scrapers[source_name]
                    
                    if source_name == "Google Scholar":
                        results = scraper.search(query_adv, num_results=num_results_adv, db=db, review_only=True)
                    else:
                        results = scraper.search(query_adv, max_results=num_results_adv)
                        
                        session = SearchSession(
                            query=f"{query_adv} [{source_name}]",
                            num_results=len(results),
                            successful_downloads=0,
                            status='ACTIVE'
                        )
                        db.add(session)
                        db.commit()
                        db.refresh(session)
                        
                        for result in results:
                            is_duplicate = False
                            
                            if result.get('doi'):
                                exists = db.query(Article).filter(Article.doi == result['doi']).first()
                                if exists:
                                    is_duplicate = True
                            
                            if not is_duplicate:
                                normalized_title = re.sub(r'[^\w\s]', '', result['title'].lower())
                                all_articles = db.query(Article).all()
                                for art in all_articles:
                                    art_normalized = re.sub(r'[^\w\s]', '', art.title.lower())
                                    if art_normalized == normalized_title:
                                        is_duplicate = True
                                        break
                            
                            if not is_duplicate:
                                article = Article(
                                    title=result['title'],
                                    authors=result.get('authors'),
                                    year=result.get('year'),
                                    source=result.get('source'),
                                    link=result.get('link'),
                                    doi=result.get('doi'),
                                    abstract=result.get('abstract'),
                                    pdf_path=result.get('pdf_path'),
                                    full_text=result.get('full_text'),
                                    text_extraction_status=result.get('extraction_status', 'NOT_ATTEMPTED'),
                                    extraction_method=result.get('extraction_method'),
                                    status="IDENTIFIED",
                                    search_session_id=session.id
                                )
                                db.add(article)
                        
                        db.commit()
                    
                    all_results.append({
                        'source': source_name,
                        'count': len(results)
                    })
                    
                except Exception as e:
                    st.warning(f"Erreur avec {source_name}: {e}")
                    all_results.append({'source': source_name, 'count': 0})
            
            progress_bar.progress(1.0)
            status_text.text("Recherche termin√©e!")
            
            df_summary = pd.DataFrame(all_results)
            st.dataframe(df_summary, use_container_width=True, hide_index=True)
            
            total = sum([r['count'] for r in all_results])
            total = sum([r['count'] for r in all_results])
            st.metric("Total d'articles trouv√©s", total)

            # --- Lancement Analyse IA en arri√®re-plan ---
            if total > 0:
                st.toast("Lancement de l'analyse IA en arri√®re-plan...")
                
                def run_background_analysis(session_ids, query):
                    ranker = AdvancedRanker()
                    db_thread = next(get_db())
                    all_new_ids = []
                    for sess_id in session_ids:
                        arts = db_thread.query(Article).filter(Article.search_session_id == sess_id).all()
                        all_new_ids.extend([a.id for a in arts])
                    db_thread.close()
                    
                    if all_new_ids:
                        ranker.process_batch_and_update_db(all_new_ids, query)
                
                # R√©cup√©rer les IDs des sessions cr√©√©es
                # Note: On doit r√©cup√©rer les IDs des sessions cr√©√©es dans la boucle
                # Simplification: On relance une requ√™te pour trouver les sessions actives r√©centes ou on stocke les IDs
                # Ici on va faire simple : on lance l'analyse sur tous les articles IDENTIFIED sans score
                
                def run_global_analysis_on_missing():
                    ranker = AdvancedRanker()
                    db_thread = next(get_db())
                    # Articles sans score
                    articles_to_process = db_thread.query(Article).filter(
                        Article.status == "IDENTIFIED",
                        Article.relevance_score == None
                    ).all()
                    ids = [a.id for a in articles_to_process]
                    db_thread.close()
                    
                    if ids:
                        ranker.process_batch_and_update_db(ids, query_adv)

                thread = threading.Thread(target=run_global_analysis_on_missing)
                thread.start()
                
                # Lancer r√©cup√©ration PDFs manquants (pour toutes les sessions)
                from src.pdf_retriever import auto_retrieve_missing_pdfs
                pdf_thread = threading.Thread(target=auto_retrieve_missing_pdfs, args=(None, 100))
                pdf_thread.start()
                
                st.toast("‚úì Analyse IA + R√©cup√©ration PDFs d√©marr√©es en arri√®re-plan !")

db.close()
